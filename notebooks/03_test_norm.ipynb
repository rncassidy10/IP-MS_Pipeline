{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Test Normalization Function\n",
    "This notebook tests `norm_ip()` for data normalization and imputation.\n",
    "\n",
    "**Input:** Loads `data_after_qc.pkl` (QC-approved samples)  \n",
    "**Output:** Saves `data_after_norm.pkl` (normalized data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check_after",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from ipms.analysis import load_data, norm_ip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"✓ All imports successful!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cell 2: Load QC-Approved Data\n",
    "data = load_data('../results/data_after_qc.pkl')\n",
    "\n",
    "print(f\"\\n✓ Loaded {data['metadata']['n_proteins']} proteins\")\n",
    "print(f\"Samples: {data['metadata']['n_samples']}\")\n",
    "\n",
    "if 'samples_dropped' in data['metadata'] and data['metadata']['samples_dropped']:\n",
    "    print(f\"\\nNote: {len(data['metadata']['samples_dropped'])} samples were dropped during QC\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cell 3: Check Data Before Normalization\n",
    "df = data['df']\n",
    "intensity_cols = data['intensity_cols']\n",
    "\n",
    "# Get all intensity columns\n",
    "all_cols = []\n",
    "for cols in intensity_cols.values():\n",
    "    all_cols.extend(cols)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA BEFORE NORMALIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nIntensity ranges (raw values):\")\n",
    "for condition, cols in intensity_cols.items():\n",
    "    values = df[cols].values.flatten()\n",
    "    values = values[~pd.isna(values)]\n",
    "    print(f\"  {condition}:\")\n",
    "    print(f\"    Min: {values.min():.1f}\")\n",
    "    print(f\"    Max: {values.max():.1f}\")\n",
    "    print(f\"    Median: {np.median(values):.1f}\")\n",
    "\n",
    "print(f\"\\nMissing values: {df[all_cols].isna().sum().sum()} total\")\n",
    "\n",
    "\n",
    "# Cell 4: Run Normalization (Default - Recommended)\n",
    "# log2 + mindet\n",
    "\n",
    "\n",
    "data = norm_ip(data, method='log2', imputation='mindet')\n",
    "\n",
    "print(\"\\n✓ Normalization complete!\")\n",
    "print(\"\\nData automatically saved to: results/data_after_norm.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "# Cell 6: Check Data After Normalization\n",
    "df = data['df']\n",
    "intensity_cols = data['intensity_cols']\n",
    "\n",
    "# Get all intensity columns\n",
    "all_cols = []\n",
    "for cols in intensity_cols.values():\n",
    "    all_cols.extend(cols)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA AFTER NORMALIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nMethod used: {data['normalization']['method']}\")\n",
    "print(f\"Imputation used: {data['normalization']['imputation']}\")\n",
    "\n",
    "print(f\"\\nIntensity ranges (normalized):\")\n",
    "for condition, cols in intensity_cols.items():\n",
    "    values = df[cols].values.flatten()\n",
    "    values = values[~pd.isna(values)]\n",
    "    print(f\"  {condition}:\")\n",
    "    print(f\"    Min: {values.min():.2f}\")\n",
    "    print(f\"    Max: {values.max():.2f}\")\n",
    "    print(f\"    Median: {np.median(values):.2f}\")\n",
    "    print(f\"    Std: {np.std(values):.2f}\")\n",
    "\n",
    "missing_after = df[all_cols].isna().sum().sum()\n",
    "print(f\"\\nMissing values after imputation: {missing_after}\")\n",
    "\n",
    "if missing_after == 0:\n",
    "    print(\"✓ All missing values imputed!\")\n",
    "else:\n",
    "    print(f\"⚠ Still {missing_after} missing values\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cell 7: Check Normalization Plots\n",
    "import os\n",
    "\n",
    "plot_path = '../results/figures/qc/normalization_comparison.pdf'\n",
    "\n",
    "if os.path.exists(plot_path):\n",
    "    size_kb = os.path.getsize(plot_path) / 1024\n",
    "    print(f\"✓ Normalization comparison plot created!\")\n",
    "    print(f\"  Location: {plot_path}\")\n",
    "    print(f\"  Size: {size_kb:.1f} KB\")\n",
    "    print(f\"\\nOpen this plot to see before/after distributions!\")\n",
    "else:\n",
    "    print(f\"✗ Plot not found: {plot_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cell 8: Summary Statistics\n",
    "print(\"=\"*60)\n",
    "print(\"NORMALIZATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nProteins: {data['metadata']['n_proteins']}\")\n",
    "print(f\"Samples: {data['metadata']['n_samples']}\")\n",
    "print(f\"Conditions: {data['metadata']['conditions']}\")\n",
    "\n",
    "print(f\"\\nNormalization applied:\")\n",
    "print(f\"  Method: {data['normalization']['method']}\")\n",
    "print(f\"  Imputation: {data['normalization']['imputation']}\")\n",
    "\n",
    "print(f\"\\nData saved to: results/data_after_norm.pkl\")\n",
    "print(f\"Plots saved to: results/figures/qc/normalization_comparison.pdf\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEP: 04_test_stat.ipynb for statistical analysis\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Alternative Normalization Methods (Optional)\n",
    "#### Uncomment ONE of these to try different methods\n",
    "\n",
    "#### Z-score normalization with KNN imputation\n",
    "# data = norm_ip(data, method='zscore', imputation='knn')\n",
    "\n",
    "#### Quantile normalization with median imputation\n",
    "# data = norm_ip(data, method='quantile', imputation='median')\n",
    "\n",
    "#### Median normalization with zero imputation\n",
    "# data = norm_ip(data, method='median', imputation='zero')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "notes",
   "metadata": {},
   "source": [
    "## About the Methods\n",
    "\n",
    "### Normalization Methods:\n",
    "- **log2** (default): Log2 transformation - standard for proteomics, reduces dynamic range\n",
    "- **zscore**: Z-score normalization - mean=0, std=1 per sample\n",
    "- **quantile**: Quantile normalization - makes distributions identical\n",
    "- **median**: Median normalization - centers samples on global median\n",
    "\n",
    "### Imputation Methods:\n",
    "- **mindet** (default): Minimum detection - imputes with (min - 1.8×std) per sample. Standard for proteomics (assumes missing = low abundance)\n",
    "- **zero**: Replace with 0 - conservative, treats missing as absent\n",
    "- **median**: Replace with sample median - middle ground\n",
    "- **knn**: K-nearest neighbors - uses similar proteins to estimate values\n",
    "\n",
    "### Recommendation:\n",
    "**For IP-MS: Use `method='log2', imputation='mindet'`** (the default)\n",
    "\n",
    "This is the standard in the field and assumes missing values represent low-abundance proteins below detection limit.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "After normalization:\n",
    "1. ✓ Review the normalization_comparison.pdf plot\n",
    "2. ✓ Check that distributions look reasonable\n",
    "3. ✓ Proceed to **04_test_stat.ipynb** for statistical analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
